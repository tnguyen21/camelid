# modded-llama2

speedrun of llama2 7B training on fineweb to <3.28 val loss

heavily inspired by [@karpathy's llama.c](https://github.com/karpathy/llama2.c/blob/master/train.py) and [@KellerJordan's modded-nanogpt](https://github.com/KellerJordan/modded-nanogpt)
